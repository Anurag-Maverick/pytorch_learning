{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport torch ","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:33:51.518453Z","iopub.execute_input":"2021-06-11T17:33:51.518804Z","iopub.status.idle":"2021-06-11T17:33:52.432362Z","shell.execute_reply.started":"2021-06-11T17:33:51.518775Z","shell.execute_reply":"2021-06-11T17:33:52.431484Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Part 1: Using only Numpy","metadata":{}},{"cell_type":"code","source":"#Dataset\nx = np.array([1,2,3,4], dtype=np.float32)\ny = np.array([2,4,6,8], dtype=np.float32) #y = x^2\n\n#forward pass\ndef forward_pass(x):\n    return x*w\n\n#mse loss\ndef loss_compute(y,yhat):\n    return np.mean((yhat-y)**2)\n\n#gradient computation\n# mse = ((yhat-y)**2)/N\n# dJ/dy = 2(yhat-y)/N\n# dy/dw = x\n# So, dJ/dw = 2x(yhat-y)/N\n#but yhat = x * w\n#So, dJ/dw = 2x(w*x - y)/N\n\ndef gradient(y,yhat,x):\n    return np.mean(np.dot(2*x, (yhat - y)))\n\nlearning_rate = 0.01\nepochs = 100\nw = 0.0\nfor ep in range(epochs):\n    ypred = forward_pass(x)\n    loss = loss_compute(y,ypred)\n    \n    if (ep%10==0):\n        print(\"Loss in epoch {} is {}\".format(ep+1,loss))\n\n    dj_dw = gradient(y,ypred,x)\n    w -= learning_rate*dj_dw","metadata":{"execution":{"iopub.status.busy":"2021-06-10T18:39:14.029614Z","iopub.execute_input":"2021-06-10T18:39:14.029934Z","iopub.status.idle":"2021-06-10T18:39:14.049062Z","shell.execute_reply.started":"2021-06-10T18:39:14.029901Z","shell.execute_reply":"2021-06-10T18:39:14.048264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Loss is decreasing with more epochs!!","metadata":{}},{"cell_type":"markdown","source":"# Part 2: Using Only Pytorch","metadata":{}},{"cell_type":"code","source":"x = torch.tensor([1,2,3,4], dtype = torch.float32)\ny = torch.tensor([2,4,6,8], dtype = torch.float32)\n\nweights = torch.tensor(0.0, dtype = torch.float32, requires_grad=True)\n\ndef forward_pass(x):\n    return x * weights\n\ndef loss_compute(y,yhat):\n    return ((yhat-y)**2).mean()\n\nlearning_rate = 0.1\nepochs = 100\nfor ep in range(epochs):\n    ypred = forward_pass(x)\n    loss = loss_compute(y,ypred)\n    loss.backward()\n    \n    if (ep%10==0):\n        print(\"Loss in epoch {} is {}\".format(ep+1,loss))\n    \n    with torch.no_grad():\n        weights -= learning_rate * weights.grad\n    weights.grad.zero_()\n    \nprint(\"Predicted value of 7.5 is {}\".format(forward_pass(7.5)))","metadata":{"execution":{"iopub.status.busy":"2021-06-10T18:39:14.050599Z","iopub.execute_input":"2021-06-10T18:39:14.051014Z","iopub.status.idle":"2021-06-10T18:39:14.140648Z","shell.execute_reply.started":"2021-06-10T18:39:14.050981Z","shell.execute_reply":"2021-06-10T18:39:14.139597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pytorch Training Pipeline: Model, Loss, optimizer","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:33:55.797450Z","iopub.execute_input":"2021-06-11T17:33:55.797820Z","iopub.status.idle":"2021-06-11T17:33:55.802120Z","shell.execute_reply.started":"2021-06-11T17:33:55.797793Z","shell.execute_reply":"2021-06-11T17:33:55.801191Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"x = torch.tensor([[1],[2],[3],[4]], dtype=torch.float32)\ny = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n\nn_samples, n_features = x.shape\n\ninput_size = n_features\noutput_size = n_features\n\nmodel = nn.Linear(input_size, output_size)\n\nloss = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr = 0.01) #Adam or RMSprop\nepochs = 1000\n\nfor ep in range(epochs):\n    ypred = model(x)\n    \n    l = loss(y,ypred)    \n    l.backward()\n    \n    if (ep%100==0):\n        [w,b] = model.parameters()\n        print(\"Loss in epoch {} is {}\".format(ep+1,l))\n    \n    optimizer.step()       #gradient update\n    optimizer.zero_grad()  #zero gradients\n    \ntest_sample = torch.tensor([7.5], dtype=torch.float32)\n    \nprint(model(test_sample).item())\nprint(\"Final parameters: {},{}\".format(w,b))","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:44:06.013364Z","iopub.execute_input":"2021-06-11T17:44:06.013674Z","iopub.status.idle":"2021-06-11T17:44:06.415380Z","shell.execute_reply.started":"2021-06-11T17:44:06.013647Z","shell.execute_reply":"2021-06-11T17:44:06.414660Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Loss in epoch 1 is 24.039939880371094\nLoss in epoch 101 is 3.41038179397583\nLoss in epoch 201 is 0.3336895704269409\nLoss in epoch 301 is 0.16478882730007172\nLoss in epoch 401 is 0.13933369517326355\nLoss in epoch 501 is 0.11622396856546402\nLoss in epoch 601 is 0.0945805013179779\nLoss in epoch 701 is 0.07511672377586365\nLoss in epoch 801 is 0.05822810158133507\nLoss in epoch 901 is 0.04404669255018234\n14.284467697143555\nFinal parameters: Parameter containing:\ntensor([[1.8461]], requires_grad=True),Parameter containing:\ntensor([0.4386], requires_grad=True)\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}